---
title       : "Sentiment Analysis"
subtitle    : "감정사전 & 감정점수 만들기"
author      : "김형준"
job         : "Analytic Director / (주) 퀀트랩 / kim@mindscale.kr "
logo        : logo_03.png
license     : by-nc-sa
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
widgets     : [mathjax]            # {mathjax, bootstrap, quiz}
mode        : selfcontained
hitheme     : tomorrow      # {tomorrow, tomorrow_night, solarized_dark, solarized_light}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
knit        : slidify::knit2slides

---

## 오늘의 목표  

<h3b> - 감정 사전 만들기</h3b>  
<h3b> - 감정 점수 만들기</h3b>  

<h3b> - 상관관계 이해하기</h3b>  
<h3b> - 회귀분석 이해하기</h3b>  
<h3b> - 모형평가 이해하기</h3b>  

--- .newbackground

## 왜 감정분석을 하는가?

<h3b> 설문지의 단점</h3b>  
<h3b> 1) 조사 비용 발생 </h3b>  
<h3b> 2) 미리 정해진 문항만 측정 가능</h3b>  
<h3b> 3) 사회적 바람직성 등 편향 발생</h3b>  

--- .newbackground

## 감정분석

<h3b> 텍스트에서 감정 단어를 추출하여 점수화</h3b>  
<h3b> 1) 기계 학습 (Machine Learning) </h3b>  
<h3b> 2) 단어 사전 기반 </h3b>

--- .newbackground

## 사전 기반 분석

<h3b> 장점 </h3b>  
<h3b> - 사용하기 간편 </h3b>  
<h3b> 단점 </h3b>  
<h3b> - 주제에 따라 사전이 달라 짐 </h3b>  
<h3b> - 동음이의어 처리 힘듦 e.g) bank </h3b>  

--- .newbackground

## 기계학습 기반 분석

<h3b> 장점 </h3b>  
<h3b> - 높은 정확도  </h3b>  
<h3b> 단점 </h3b>  
<h3b> - Over-fitting 해결 </h3b>  
<h3b> - 많은 데이터 필요 </h3b>  
<h3b> 예) 나이브 베이즈 / 최대 엔트로피 / 서포트벡터머신 / </h3b>  
<h3b>  랜덤 포레스트 / 토픽 모델 </h3b>

--- .newbackground

## 감정 분석 예시

<center><img src="assets/img/election.png" height=450px width=800px></center>

--- .newbackground

## 감정 분석 예시

<center><img src="assets/img/twitter.png" height=450px width=800px></center>


--- .newbackground

## 사전 지식 

<h3b> 감정분석: 문장에 사용된 단어로 감정을 예측 </h3b> 

<h3b>예: "이 영화는 좀 길지만 재미있고 신난다"  </h3b>    
<h3b> - 길다 -> 부정 </h3b>    
<h3b> - 재미있다 -> 긍정 </h3b>    
<h3b> - 신나다 -> 긍정  </h3b>    

--- .newbackground

## 예측 분석 

<h3b> 예측분석 </h3b> 

<h3b> 선형회귀분석 </h3b>    
<h3b> SVM </h3b>    
<h3b> RandomForest </h3b>    
<h3b> Deep Learning  </h3b>    

--- .newbackground


## 회귀분석(선형(직선) 모형) 

<h3b> 예시 </h3b>

<h3b>- 키가 1cm 증가할 때마다 몸무게가 1kg 증가  </h3b>  
<h3b>- 월 소득이 100만원 증가할 때마다 몸무게가 1kg 감소 </h3b>   
<h3b>- 부정단어가 1개 증가할 때 마다 평점 1점 감점  </h3b>  
<h3b>- 긍정단어가 1개 증가할 때 마다 평점 1점 증가  </h3b>  

--- .newbackground

## 회귀분석의 문제 

<h3b>- 변수가 많아지면 과적합(overfitting)이 발생  </h3b>  
<h3b>- 회귀계수가 극단적으로 커지거나 작아짐 </h3b>   
<h3b>- 예측력이 떨어짐  </h3b>  
<h3b>- 과적합을 막아주는 방법이 필요  </h3b>  

--- .newbackground

## 과적합을 막는 법 

<h3b>- 라쏘(lasso): 작은 회귀계수를 0으로 만듦   </h3b>  
<h3b>- 릿지(ridge): 전반적으로 회귀계수를 줄여줌 </h3b>   
<h3b>- 엘라스틱넷(elastic net): 라쏘 + 릿지  </h3b>  
<h3b>- 감정분석에서 라쏘를 쓰면 감정 단어만 추출됨 </h3b>  

--- &twocol .modal

## 상관관계

*** =left

```{r, echo = F}
x <- 1:10
y <- 1:10
```

```{r, echo=F,warning=F, fig.width=6, fig.height=6}
plot(x, y, cex.lab = 2)
cor(x, y)
```

*** =right

```{r, echo = F}
x <- 1:10
y <- 1:10
y[2] <- 10
y[8] <- 3
```

```{r, echo=F,warning=F, fig.width=6, fig.height=6}
plot(x, y, cex.lab = 2)
cor(x, y)
```

--- .newbackground .modal

## 상관관계

<h3b> x가 증가(혹은 감소)할때 y가 선형적으로 증가(혹은 감소)하는 정도 </h3b>

<h3b> scale </h3b>

<h4b> 키가 만약 cm라면, 키가 1cm 증가하면 몸무게는 1kg증가  </h4b>   
<h4b> 키가 만약 mm라면, 키가 1mm 증가하면 몸무게는 0.1kg 증가  </h4b>   

<h3b> -> 표준화해야 한다 </h3b>

--- &twocol

## 상관관계 및 회귀분석

*** =left

```{r, echo = F}
set.seed(1)
heights = rnorm(30,180,5)
heights = sort(heights, decreasing = F)
weights =  -10 + heights*.5 + rnorm(30,0,2)
```

```{r, echo=F,warning=F, fig.width=6}
plot(heights, weights, pch = 16, ylim = c(70,90), cex.lab = 2)
abline(lm(weights ~ heights), col="red")
```

*** =right

```{r results="asis", echo=F, fig.width=5, message=F}
library(xtable)
library(lm.beta)
print(xtable(coef(summary(lm(weights ~ heights)))),type="html")
```

```{r}
cor(weights, heights)
```

--- &twocol

## X가 2개라면?

*** =left

```{r, echo = F}
set.seed(1)
heights = rnorm(30,180,5)
heights = sort(heights, decreasing = F)
weights =  -10 + heights*.5 + rnorm(30,0,2)
iq <- 87 + heights*0 + rnorm(30,0,5)
```

```{r, echo=F,warning=F, fig.width=6, fig.height=6, message=F}
library(scatterplot3d)
test <- scatterplot3d(heights,iq,weights,color='red',
                      pch=19,col.grid='lightblue',type='h', cex.lab = 2)

my.lm<-lm(weights~heights+iq)
test$plane3d(my.lm,col='blue')
cor(weights, heights)
```

*** =right

```{r, echo=F,warning=F, fig.width=6, fig.height=6}
test <- scatterplot3d(iq,heights,weights,color='red',
                      pch=19,col.grid='lightblue',type='h', cex.lab = 2)

my.lm<-lm(weights~iq+heights)
test$plane3d(my.lm,col='blue')
cor(weights, iq)
```

--- .newbackground

## 다중회귀분석

```{r results="asis", echo=F, fig.width=5}
my.lm<-lm(weights~iq+heights)
print(xtable(coef(summary(lm(weights ~ iq + heights)))),type="html")
```

--- .newbackground

## 예측력  

<h3b> MSE(Mean of Square Error) </h3b>
$$ MSE = \sum_{i=1}^{n}(Y_{i} - \hat{Y_{i}})^{2} $$

<h3b> 정확도(Accracy) </h3b>  

<center><img src="assets/img/confusionMat.jpg" height=200px width=600px></center>
<h3b> 정확도 = (TP + TN) / (TP + FP + TN + FN) </h3b>

--- &twocol

## Traninig Vs Test

*** =left

```{r results="asis", echo=F, fig.width=6}
lm.1 <- lm(weights~heights)
lm.2 <- lm(weights~iq+heights)
plot(weights ~ heights, pch = 3, lwd = 5, main = "Sample(Training) Set", cex.lab = 2) 

points((-10 + heights*.5) ~ heights, pch = 15, col = "red")
points(predict(lm.1) ~ heights, pch = 15,  col = "blue")
points(predict(lm.2) ~ heights , pch = 16,  col = "green")

a <- apply(cbind(heights, heights, weights, predict(lm.2)),1, function(coords){lines(coords[1:2],coords[3:4],lty=2)})



true_mse = sum((weights - (-10 + .5*heights))^2)
lm_mse = sum((weights - predict(lm.1))^2)
lm2_mse = sum((weights - predict(lm.2))^2)

legend("topleft",legend=c(paste("True MSE:",round(true_mse,2)),
                       paste("LM MSE:",round(lm_mse,2)), 
                       paste("LM2 MSE:",round(lm2_mse,2))),
                         col=c("red","blue","green"),lty=1)

```

*** =right

```{r results="asis", echo=F, fig.width=6}
set.seed(12345)
t.heights = rnorm(30,180,5)
t.weights =  -10 + t.heights*.5 + rnorm(30,0,2)
plot(t.weights ~ t.heights, pch = 3, lwd = 5, main = "Sample(Test) Set")
points((-10 + t.heights*.5) ~ t.heights, pch = 15,  col = "red", cex.lab = 2)
points(predict(lm.1) ~ t.heights, pch = 15,  col = "blue")
points(predict(lm.2) ~ t.heights , pch = 16,  col = "green")

a <- apply(cbind(t.heights, t.heights, t.weights, predict(lm.2)),1, function(coords){lines(coords[1:2],coords[3:4],lty=2)})

true_mse = sum((t.weights - (-10 + .5*t.heights))^2)
lm_mse = sum((t.weights - predict(lm.1))^2)
lm2_mse = sum((t.weights - predict(lm.2))^2)

legend("topleft",legend=c(paste("True MSE:",round(true_mse,2)),
                       paste("LM MSE:",round(lm_mse,2)), 
                       paste("LM2 MSE:",round(lm2_mse,2))),
                         col=c("red","blue","green"),lty=1)

```

--- .newbackground

## Over-fitting

<center><img src="assets/img/lambda.png" height=450px width=700px></center>

--- .newbackground

## Over-fitting(과적합)

<h3b> How to avoid Over-fitting </h3b>  
<h3b>- Penality of Model Complexity (MSE 보정)  </h3b>  
<h3b><font color="red">- Regulization (Lasso, Ridge, Elastic Net) </font>  </h3b>  
<h3b>- Bayesian  </h3b>  
<h3b>- Drop Out, Bagging, Feature Bagging  </h3b>  

--- .newbackground

## Lasso Vs Ridge

<center><img src="assets/img/lassoridge.png" height=450px width=800px></center>

--- &twocol

## Lasso Vs Ridge

*** =left

```{r, echo = F, message=F}
library(glmnet)
```

```{r, echo = F, fig.width=6}
#cv.res <- cv.glmnet(cbind(heights, iq), weights, family = "gaussian", alpha = 1)
#cv.coef <- coef(cv.res, s = "lambda.min")
#plot(cv.res)
# 
#lasso.res <- glmnet(cbind(heights, iq), weights, lambda = cv.res$lambda.min, alpha = 1)
#plot(coefficients(lasso.res))
#lasso_mse = sum((t.weights - predict(lasso.res, cbind(heights, iq)))^2)
#lasso_mse

lbs_fun <- function(fit, ...) {
        L <- length(fit$lambda)
        x <- log(fit$lambda[L])
        y <- fit$beta[, L]
        labs <- names(y)
        text(x - .3*x, y , labels=labs, pos = 3, offset = .5, ...)
        legend('topright', legend=labs, col=1:length(labs), lty=1) # <<< ADDED BY ME
}

lasso.res <- glmnet(cbind(heights, iq), weights, alpha = 1)
plot(lasso.res, xvar="lambda", col=1:dim(coef(lasso.res))[1], main = "LASSO", ylim = c(0, .6), cex.lab = 2)
lbs_fun(lasso.res)

```

*** =right

```{r, echo = F, fig.width=6}

ridge.res <- glmnet(cbind(heights, iq), weights, alpha = 0)
plot(ridge.res, xvar="lambda", col=1:dim(coef(lasso.res))[1], main = "RIDGE", ylim = c(0, .6), cex.lab = 2)
lbs_fun(ridge.res)
```

--- .newbackground

## 감정분석

<h3b> Data </h3b>  
<h3b> 아마존 모바일 폰 리뷰 중에서 2,000개만 </h3b>  

--- .newbackground

## 5. 예제 데이터 불러오기

```{r, echo = F}
setwd("~/Dropbox/repo/r_basic/sentiment_analysis/data/")
mobile <- read.csv('mobile2014.csv', stringsAsFactors = F)
```

```{r, eval = F}
mobile <- read.csv('mobile2014.csv', stringsAsFactors = F)
```

```{r}
dim(mobile)
table(mobile$Sentiment)
```

--- .newbackground

## 6. DocumentTermMatrix 만들기

```{r, warning=F, message=F}
library(tm)
```


```{r}
corpus <- Corpus(VectorSource(mobile$Texts))

##  제거할 단어 목록 확인
stopwords()
stopwords("SMART")

dtm <- DocumentTermMatrix(corpus,
                          control = list(tolower = T,
                                         removePunctuation = T,
                                         removeNumbers = T,
                                         stopwords = stopwords("SMART"),
                                         weighting = weightTfIdf))

dtm
```

--- .newbackground

## 7. 회귀분석으로 감정 사전 만들기

```{r, warning=F, message=F}
library(glmnet)
```

```{r}
X <- as.matrix(dtm)
Y <- mobile$Sentiment
```

```{r}
res.lm <- glmnet(X, Y, family = "binomial", lambda = 0) 
```

--- .newbackground
    
## 7. 회귀분석으로 감정 사전 만들기
    
```{r}
coef.lm <- coef(res.lm)[,1]
pos.lm <- coef.lm[coef.lm > 0]
neg.lm <- coef.lm[coef.lm < 0]
pos.lm <- sort(pos.lm, decreasing = T)
neg.lm <- sort(neg.lm, decreasing = F)
```

